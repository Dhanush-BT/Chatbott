{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanush-BT/DocuMind-ChatBot/blob/master/Copy_of_Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "y_ncg6CZliCi",
        "outputId": "912722c7-f60b-4ea0-e2b0-a28a5b96eea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colab-xterm in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (6.4.2)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Launching Xterm..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(10000, {'cache': true}));\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv2ljFjhIiaP"
      },
      "source": [
        "    curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4fnRR78Hs3_"
      },
      "source": [
        "ollama serve & ollama pull gemma2:2b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P1wvoJ7PMBQa",
        "outputId": "78013196-0e8a-432e-a00a-b6bad2364042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.11.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install ollama\n",
        "!pip install PyMuPDF\n",
        "!pip install python-docx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "aPra2uVtMNaA",
        "outputId": "61d89e64-7939-4b73-b16e-e4ab0231f93f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-15-3219739681.py:198: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1859255d03339ae719.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://1859255d03339ae719.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import fitz\n",
        "from docx import Document\n",
        "import json\n",
        "import ollama\n",
        "from typing import Dict, Any\n",
        "import time\n",
        "import psutil\n",
        "import gradio as gr\n",
        "\n",
        "def print_resource_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_mb = process.memory_info().rss / (1024 ** 2)\n",
        "    cpu_percent = psutil.cpu_percent(interval=0.1)\n",
        "    print(f\" Memory: {mem_mb:.2f} MB |  CPU: {cpu_percent:.2f}%\")\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    doc = fitz.open(file_path)\n",
        "    text = ''\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_docx(file_path: str) -> str:\n",
        "    doc = Document(file_path)\n",
        "    text = '\\n'.join([para.text for para in doc.paragraphs])\n",
        "    return text.strip()\n",
        "\n",
        "def extract_file_to_json(file_path: str) -> Dict[str, Any]:\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext == '.pdf':\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "    elif ext == '.docx':\n",
        "        text = extract_text_from_docx(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Only PDF and DOCX are supported.\")\n",
        "\n",
        "    return {\n",
        "        \"filename\": os.path.basename(file_path),\n",
        "        \"filepath\": file_path,\n",
        "        \"content\": text\n",
        "    }\n",
        "\n",
        "def chunk_text(text: str, max_chunk_size: int = 3500, overlap_size: int = 200):\n",
        "    \"\"\"\n",
        "    Splits text into overlapping chunks for LLM processing.\n",
        "    :param text: Full document text\n",
        "    :param max_chunk_size: Maximum chunk size (characters)\n",
        "    :param overlap_size: Number of overlapped characters between chunks\n",
        "    :return: List of chunk strings\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_length = len(text)\n",
        "    while start < text_length:\n",
        "        end = start + max_chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap_size\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "    return chunks\n",
        "\n",
        "def ask_query_on_chunks(doc_text: str, question: str):\n",
        "    chunks = chunk_text(doc_text)\n",
        "    answers = []\n",
        "    total_chunks = len(chunks)\n",
        "    matched_chunk_index = None\n",
        "    chunk_used_length = 0\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        system_prompt = (\n",
        "            \"You are a helpful assistant. Only answer questions strictly based on the document provided below. \"\n",
        "            \"If the answer is not in the document, say 'The document does not contain that information.' \"\n",
        "            \"You can support greeting or casual conversation inputs, but always prioritize the document content.\\n\\n\"\n",
        "            f\"DOCUMENT:\\n{chunk}\\n\\n\"\n",
        "            f\"QUESTION: {question}\"\n",
        "        )\n",
        "        response = ollama.chat(\n",
        "            model='gemma2:2b',\n",
        "            messages=[{\"role\": \"user\", \"content\": system_prompt}]\n",
        "        )\n",
        "        answer = response['message']['content'].strip()\n",
        "        answers.append(answer)\n",
        "\n",
        "        if answer and \"does not contain\" not in answer:\n",
        "            matched_chunk_index = i\n",
        "            chunk_used_length = len(chunk)\n",
        "            return answer, {\n",
        "                \"matched_chunk\": i + 1,\n",
        "                \"total_chunks\": total_chunks,\n",
        "                \"chunk_length\": chunk_used_length,\n",
        "                \"source\": \" Matched\"\n",
        "            }\n",
        "\n",
        "    # If no good match found\n",
        "    return \"The answer was not found in the document.\", {\n",
        "        \"matched_chunk\": None,\n",
        "        \"total_chunks\": total_chunks,\n",
        "        \"chunk_length\": 0,\n",
        "        \"source\": \" Fallback (No match found)\"\n",
        "    }\n",
        "\n",
        "\n",
        "# Global variable to store document content\n",
        "doc_content = \"\"\n",
        "current_filename = \"\"\n",
        "\n",
        "def upload_file(file):\n",
        "    global doc_content, current_filename\n",
        "    if file is None:\n",
        "        return \"No file uploaded\", \"Please upload a PDF or DOCX file to start chatting.\"\n",
        "\n",
        "    try:\n",
        "        file_data = extract_file_to_json(file.name)\n",
        "        doc_content = file_data['content']\n",
        "        current_filename = file_data['filename']\n",
        "        return f\"Document '{current_filename}' loaded successfully!\", f\" **{current_filename}**\\n\\n **Document loaded and ready for questions!**\"\n",
        "    except Exception as e:\n",
        "        return f\"Error loading file: {str(e)}\", \"Please try uploading a different file.\"\n",
        "\n",
        "def chat_with_document(message, history):\n",
        "    global doc_content\n",
        "\n",
        "    if not doc_content:\n",
        "        return history + [[message, \" Please upload a PDF or DOCX document using the file upload area above before asking questions.\"]]\n",
        "\n",
        "    if not message.strip():\n",
        "        return history + [[message, \"Please enter a question.\"]]\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        answer, metadata = ask_query_on_chunks(doc_content, message)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Resource usage\n",
        "        process = psutil.Process(os.getpid())\n",
        "        mem_mb = process.memory_info().rss / (1024 ** 2)\n",
        "        cpu_percent = psutil.cpu_percent(interval=0.1)\n",
        "        response_time = end_time - start_time\n",
        "\n",
        "        # Metadata markdown block\n",
        "        meta_info = (\n",
        "            f\"\\n---\\n\"\n",
        "            f\" **Answer Source:** {metadata['source']}\\n\"\n",
        "            f\" **Chunk Used:** {metadata['matched_chunk']}/{metadata['total_chunks'] if metadata['total_chunks'] else 'N/A'}\\n\"\n",
        "            f\" **Chunk Length:** {metadata['chunk_length']} characters\\n\"\n",
        "            f\" **CPU Usage:** {cpu_percent:.2f}% &nbsp;&nbsp;&nbsp; **Memory:** {mem_mb:.2f} MB\\n\"\n",
        "            f\" **Response Time:** {response_time:.2f} seconds\"\n",
        "        )\n",
        "\n",
        "        return history + [[message, answer + meta_info]]\n",
        "\n",
        "    except Exception as e:\n",
        "        return history + [[message, f\" Error processing your question: {str(e)}\"]]\n",
        "\n",
        "\n",
        "def clear_chat():\n",
        "    return []\n",
        "\n",
        "def get_document_info():\n",
        "    global current_filename, doc_content\n",
        "    if not doc_content:\n",
        "        return \"No document loaded\"\n",
        "\n",
        "    word_count = len(doc_content.split())\n",
        "    char_count = len(doc_content)\n",
        "    return f\" **{current_filename}**\\n\\n **Document Stats:**\\n- Characters: {char_count:,}\\n- Words: {word_count:,}\\n- Estimated reading time: {word_count // 200} minutes\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"DocuMind\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.HTML(\"<h1 style='text-align: center; color: #2563eb;'> DocuMind </h1>\")\n",
        "    gr.HTML(\"<p style='text-align: center; color: #64748b;'>Upload a PDF or DOCX file and ask questions about its content</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # File upload section\n",
        "            gr.HTML(\"<h3> Upload Document</h3>\")\n",
        "            file_input = gr.File(\n",
        "                label=\"Select PDF or DOCX file\",\n",
        "                file_types=[\".pdf\", \".docx\"],\n",
        "                file_count=\"single\"\n",
        "            )\n",
        "            upload_status = gr.Textbox(\n",
        "                label=\"Upload Status\",\n",
        "                interactive=False,\n",
        "                lines=1,\n",
        "                value=\"No file uploaded yet...\"\n",
        "            )\n",
        "\n",
        "            # Document info section\n",
        "            gr.HTML(\"<h3> Document Info</h3>\")\n",
        "            doc_info = gr.Markdown(\"No document loaded\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            # Chat section\n",
        "            gr.HTML(\"<h3> Chat with Document</h3>\")\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=400,\n",
        "                show_label=False\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg_input = gr.Textbox(\n",
        "                    placeholder=\"Ask a question about your document...\",\n",
        "                    show_label=False,\n",
        "                    scale=4\n",
        "                )\n",
        "                submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            clear_btn = gr.Button(\" Clear Chat\", variant=\"secondary\", size=\"sm\")\n",
        "\n",
        "    # Event handlers\n",
        "    file_input.upload(\n",
        "        fn=upload_file,\n",
        "        inputs=[file_input],\n",
        "        outputs=[upload_status, doc_info]\n",
        "    )\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=chat_with_document,\n",
        "        inputs=[msg_input, chatbot],\n",
        "        outputs=[chatbot]\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=[msg_input]\n",
        "    )\n",
        "\n",
        "    msg_input.submit(\n",
        "        fn=chat_with_document,\n",
        "        inputs=[msg_input, chatbot],\n",
        "        outputs=[chatbot]\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=[msg_input]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_chat,\n",
        "        outputs=[chatbot]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        debug=True,\n",
        "        share=True,\n",
        "        inbrowser=True\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO5EJ0syzsshIgeFagjTcV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}